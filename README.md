# post-training-of-LLMs

# 概述
大语言模型的训练有两个阶段：  
 **预训练阶段（pre-training）**：使模型学会预测下一个文字或标注；  
 **后训练阶段（post-training）**：进一步训练模型使得模型具有能够处理任务的能力，比如学会回答问题。
# 后训练阶段
三种后训练方法：  
**监督微调（Supervised Fine-Tuning）SFT**：通过一些标记好的数据对模型进行训练，使模型模仿输入（提示）与输出（响应）之间的关系。  
**直接偏好优化（direc preference optimization ）DPO**：让模型根据同一提示下的优质响应和劣质响应，使模型偏向优质响应远离劣质响应，帮助提高响应质量，减少不良信息。  
**在线强化学习（online RL）**：用户输入提示，模型做出响应，奖励模型再对回应进行评估，模型再根据奖励分数进行更新。

